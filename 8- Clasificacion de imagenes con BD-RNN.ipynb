{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificación de imágenes usando RNNs bidireccionales\n",
    "\n",
    "* Así como hemos utilizado CNN para clasificación de secuencias, se pueden usar las RNN para clasificación de imágenes.\n",
    "\n",
    "* Las imágenes son secuencias bidimensionales de píxels.\n",
    "\n",
    "* En las imágenes, la secuencia es espacial en vez de temporal. Eso nos permite mirar no solo el pasado y el presente, sino también el futuro. Puede parecer ciencia ficción, pero esto solo significa que para un valor de la variable que estoy recorriendo (ejemplo: x o y) no solo tengo acceso a los valores anteriores y actual $x \\leq x_0$ sino también a los valores futuros $x > x_0$.\n",
    "\n",
    "* Como en general tengo acceso a toda la secuencia, tiene sentido utilizar BD-RNN.\n",
    "\n",
    "<img src=\"tiempoyespacio.png\">\n",
    "\n",
    "* Puedo interpretar a la imagen como que una dimensión es el tiempo, y otra dimensión son los features.\n",
    "* La elección de qué dimensión es el tiempo y cual los features es arbitraria.\n",
    "\n",
    "<img src=\"tiempoyfeatures.png\">\n",
    "\n",
    "* O también, se pueden implementar ambas opciones:\n",
    "\n",
    "<img src=\"dimensiones.png\">\n",
    "\n",
    "- ¿Cómo se implementaría?\n",
    "- ¿Cómo son las dimensiones en cada punto, si tengo M unidades ocultas, un batch_size de N y la imagen tiene dimensiones WxH?\n",
    "\n",
    "* Arquitectura entera:\n",
    "\n",
    "<img src=\"BD-RNN-clasif.png\">\n",
    "\n",
    "con una capa softmax al final.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist(path, kind='train'):\n",
    "    import os\n",
    "    import gzip\n",
    "    import numpy as np\n",
    "\n",
    "    \"\"\"Load MNIST data from `path`\"\"\"\n",
    "    labels_path = os.path.join(path,\n",
    "                               '%s-labels-idx1-ubyte.gz'\n",
    "                               % kind)\n",
    "    images_path = os.path.join(path,\n",
    "                               '%s-images-idx3-ubyte.gz'\n",
    "                               % kind)\n",
    "\n",
    "    with gzip.open(labels_path, 'rb') as lbpath:\n",
    "        labels = np.frombuffer(lbpath.read(), dtype=np.uint8,\n",
    "                               offset=8)\n",
    "\n",
    "    with gzip.open(images_path, 'rb') as imgpath:\n",
    "        images = np.frombuffer(imgpath.read(), dtype=np.uint8,\n",
    "                               offset=16).reshape(len(labels), 784)\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "X_train, y_train = load_mnist('data/fashion', kind='train')\n",
    "X_test, y_test = load_mnist('data/fashion', kind='t10k')\n",
    "X_train=X_train.reshape(-1,28,28)\n",
    "X_test=X_test.reshape(-1,28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, GRU, Bidirectional, GlobalMaxPooling1D, Lambda, Concatenate, Dense, Conv2D, Reshape, Permute,Flatten\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def get_mnist(limit=None):\n",
    "  if not os.path.exists('../large_files'):\n",
    "    print(\"You must create a folder called large_files adjacent to the class folder first.\")\n",
    "  if not os.path.exists('../large_files/train.csv'):\n",
    "    print(\"Looks like you haven't downloaded the data or it's not in the right spot.\")\n",
    "    print(\"Please get train.csv from https://www.kaggle.com/c/digit-recognizer\")\n",
    "    print(\"and place it in the large_files folder.\")\n",
    "\n",
    "  print(\"Reading in and transforming data...\")\n",
    "  df = pd.read_csv('../large_files/train.csv')\n",
    "  data = df.as_matrix()\n",
    "  np.random.shuffle(data)\n",
    "  X = data[:, 1:].reshape(-1, 28, 28) / 255.0 # data is from 0..255\n",
    "  Y = data[:, 0]\n",
    "  if limit is not None:\n",
    "    X, Y = X[:limit], Y[:limit]\n",
    "  return X, Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data\n",
    "X=X_train\n",
    "Y=y_train\n",
    "# config\n",
    "D = 28\n",
    "M = 100\n",
    "\n",
    "\n",
    "# input is an image of size 28x28\n",
    "input_ = Input(shape=(D, D))\n",
    "\n",
    "# up-down\n",
    "rnn1 = Bidirectional(LSTM(M, return_sequences=True))\n",
    "x1 = rnn1(input_) # output is N x D x 2M\n",
    "x1 = GlobalMaxPooling1D()(x1) # output is N x 2M\n",
    "#x1=Reshape((1,D,2*M))(x1)\n",
    "#x1=Permute((1,3,2))(x1)\n",
    "#x1=Conv2D(1, (1,1), strides=(1, 1), padding='same')(x1)\n",
    "#x1=Flatten()(x1)\n",
    "\n",
    "\n",
    "# left-right\n",
    "rnn2 = Bidirectional(LSTM(M, return_sequences=True))\n",
    "\n",
    "# custom layer\n",
    "permutor = Lambda(lambda t: K.permute_dimensions(t, pattern=(0, 2, 1)))\n",
    "\n",
    "x2 = permutor(input_)\n",
    "x2 = rnn2(x2) # output is N x D x 2M\n",
    "x2 = GlobalMaxPooling1D()(x2) # output is N x 2M\n",
    "#x2=Reshape((1,D,2*M))(x2)\n",
    "#x2=Permute((1,3,2))(x2)\n",
    "#x2=Conv2D(1, (1,1), strides=(1, 1), padding='same')(x2)\n",
    "#x2=Flatten()(x2)\n",
    "# put them together\n",
    "concatenator = Concatenate(axis=1)\n",
    "x = concatenator([x1, x2]) # output is N x 4M\n",
    "\n",
    "# final dense layer\n",
    "output = Dense(10, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=input_, outputs=output)\n",
    "\n",
    "# testing\n",
    "# o = model.predict(X)\n",
    "# print(\"o.shape:\", o.shape)\n",
    "\n",
    "# compile\n",
    "model.compile(\n",
    "  loss='sparse_categorical_crossentropy',\n",
    "  optimizer='adam',\n",
    "  metrics=['accuracy']\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/40\n",
      "54000/54000 [==============================] - 49s 909us/step - loss: 0.5731 - acc: 0.8027 - val_loss: 0.4524 - val_acc: 0.8362\n",
      "Epoch 2/40\n",
      "54000/54000 [==============================] - 47s 869us/step - loss: 0.4115 - acc: 0.8484 - val_loss: 0.4006 - val_acc: 0.8532\n",
      "Epoch 3/40\n",
      "54000/54000 [==============================] - 47s 869us/step - loss: 0.3822 - acc: 0.8579 - val_loss: 0.3889 - val_acc: 0.8513\n",
      "Epoch 4/40\n",
      "54000/54000 [==============================] - 47s 869us/step - loss: 0.3652 - acc: 0.8641 - val_loss: 0.3844 - val_acc: 0.8548\n",
      "Epoch 5/40\n",
      "43904/54000 [=======================>......] - ETA: 8s - loss: 0.3509 - acc: 0.8693"
     ]
    }
   ],
   "source": [
    "# train\n",
    "print('Training model...')\n",
    "r = model.fit(X, Y, batch_size=64, epochs=40, validation_split=0.1)\n",
    "\n",
    "\n",
    "# plot some data\n",
    "plt.plot(r.history['loss'], label='loss')\n",
    "plt.plot(r.history['val_loss'], label='val_loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# accuracies\n",
    "plt.plot(r.history['acc'], label='acc')\n",
    "plt.plot(r.history['val_acc'], label='val_acc')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "from builtins import range, input\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, GRU, Bidirectional, Dot, Dense\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "T = 8 #Cantidad de Timesteps\n",
    "D = 2 #Cantidad de entradas por timestep\n",
    "M = 3 #Cantidad de unidades en la capa oculta\n",
    "\n",
    "\n",
    "X = np.random.randn(1, D, 2*M)\n",
    "W = np.random.randn(1, D, 1)\n",
    "input_ = Input(shape=(D, 2*M))\n",
    "output = Dot(axes, normalize=False)\n",
    "print(input)\n",
    "model = Model(inputs=input_, outputs=x)\n",
    "o, h1, c1, h2, c2 = model.predict(X)\n",
    "print(\"o:\", o)\n",
    "print(\"o.shape:\", o.shape)\n",
    "print(\"h1:\", h1)\n",
    "print(\"c1:\", c1)\n",
    "print(\"h2:\", h2)\n",
    "print(\"c2:\", c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.randn(6, D, 2*M)\n",
    "W = np.random.randn(6, 2*M, D)\n",
    "y = np.matmul(X,W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 28, 28)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflow-gpu)",
   "language": "python",
   "name": "tensorflow-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
