{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referencias:\n",
    "\n",
    "Blog de keras:\n",
    "https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html\n",
    "\n",
    "Paper de google:\n",
    "https://arxiv.org/abs/1409.3215"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequencia a Secuencia (Seq2Seq)\n",
    "https://youtu.be/HPOzAlXhuxQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![seq2seq.png](seq2seq.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sería conveniente:\n",
    "- La longitud de la secuencia de entrada pueda diferir de la de salida\n",
    "- Leer toda la secuencia de entrada antes de que empecemos a calcular la secuencia de salida\n",
    "\n",
    "¿Cual de las dos soluciones se ajusta mejor?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplicaciones:\n",
    "- Traducción\n",
    "- Chatbots\n",
    "- Speech2Text\n",
    "- Síntesis de noticias\n",
    "- Q&A\n",
    "- Image Captioning (Estrictamente la entrada no es una secuencia si no una imagen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo encoder-decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![seq2seq-teacher-forcing.png](seq2seq-teacher-forcing.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder:\n",
    "- Procesa la secuencia de entrada y devuelve su estado interno\n",
    "- Normalmente LSTM o GRU\n",
    "- ¿Cual es la diferencia entre el estado interno y la salida? En SimpleRNN y GRU coinciden, en LSTM es antes de aplicar tanh\n",
    "- Este estado interno se llama contexto y es lo que condicionara el decodificador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Se entrena para predecir la salida del próximo timestep en función del anterior.\n",
    "- El estado inicial es el estado interno del encoder.\n",
    "- Al proceso de entrenamiento se lo llama \"teacher forcing\"\n",
    "- Es decir, la salida en T+1 la obtiene en funcion de la entrada en T, partiendo del estado inicial del final del encoder\n",
    "\n",
    "\n",
    "El decoder se puede pensar como un **modelo de lenguage generador** con ciertas condiciones de entorno que generan un contexto. Ese contexto puede haber sido generado por:\n",
    "- Una imagen\n",
    "- Un texto a traducir\n",
    "- Una pregunta en Q&A\n",
    "- Una noticia\n",
    "- Una señal de voz\n",
    "Que fue procesada/o y aplicado a la LSTM o GRU del Encoder\n",
    "\n",
    "La salida será:\n",
    "- Un texto que describe la imagen\n",
    "- La traducción del texto original\n",
    "- Etc\n",
    "\n",
    "Ver video:\n",
    "https://www.youtube.com/watch?v=IxQtK2SjWWM&t=165s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por que no se usan RNN tradicional y se usa LSTM o GRU? \n",
    "\n",
    "- Se espera que en el hidden state que queda como estado inicial, quede guardado **toda la info de la oración hasta el final**. \n",
    "- Una posible solución es colocar el estado inicial en cada timestep del decoder o focalizar en un sector (**Attention**)\n",
    "- Vanilla Seq2Seq funciona bien en oraciones cortas, por eso aparece Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferencia (caracter por caracter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![seq2seq-inference.png](seq2seq-inference.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Codificamos la secuencia de entrada con el encoder para obtener el contexto\n",
    "- Utilizamos un caracter de comienzo y se lo damos como entrada al decoder junto con el contexto\n",
    "- Muestramos la salida para obtener el siguiente caracter (usamos argmax).\n",
    "- Agregamos el caracter de salida como parte de la secuencia de entrada y repetimos el proceso hasta que obtenemos el caracter de fin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo en keras: Traducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance de machine translation:\n",
    "http://www.meta-net.eu/events/meta-forum-2016/slides/09_sennrich.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modelo multilenguaje**: **Google** esta desarrollando un modelo Multilenguaje donde la traducción es de cualquier idioma a cualquier idioma. Es decir, el hidden state es lo suficiente general que puede generar traducciones razonables sin haber sido entrenado en un par de idiomas:\n",
    "\n",
    "Ingles -> Portugues\n",
    "Portugues -> Español\n",
    "\n",
    "Funciona razonablemente bien Ingles->Español y si se lo entrena con un dataset pequeño, logra performance state-of-the-art"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtenemos y formateamos datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Función que devuelve datos formateados\n",
    "def get_data(data_path = 'fra-eng/fra.txt', num_samples = 10000):\n",
    "    # num_samples: Number of samples to train on.\n",
    "    # Vectorize the data.\n",
    "    input_texts = []\n",
    "    target_texts = []\n",
    "    input_characters = set()\n",
    "    target_characters = set()\n",
    "    lines = open(data_path).read().split('\\n')\n",
    "    for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "        input_text, target_text = line.split('\\t')\n",
    "        # We use \"tab\" as the \"start sequence\" character\n",
    "        # for the targets, and \"\\n\" as \"end sequence\" character.\n",
    "        target_text = '\\t' + target_text + '\\n'\n",
    "        input_texts.append(input_text)\n",
    "        target_texts.append(target_text)\n",
    "        for char in input_text:\n",
    "            if char not in input_characters:\n",
    "                input_characters.add(char)\n",
    "        for char in target_text:\n",
    "            if char not in target_characters:\n",
    "                target_characters.add(char)\n",
    "    input_characters = sorted(list(input_characters))\n",
    "    target_characters = sorted(list(target_characters))\n",
    "    num_encoder_tokens = len(input_characters)\n",
    "    num_decoder_tokens = len(target_characters)\n",
    "    input_lenghts = [len(txt) for txt in input_texts]\n",
    "    output_lengths = [len(txt) for txt in target_texts]\n",
    "    max_encoder_seq_length = max(input_lenghts)\n",
    "    max_decoder_seq_length = max(output_lengths)\n",
    "    print('Traducción con secuencia mas larga (Notar el agregado de tab y enter):')\n",
    "    print(input_texts[np.argmax(output_lengths)])\n",
    "    print(target_texts[np.argmax(output_lengths)])\n",
    "\n",
    "    print('Number of samples:', len(input_texts))\n",
    "    print('Number of unique input tokens:', num_encoder_tokens)\n",
    "    print('Number of unique output tokens:', num_decoder_tokens)\n",
    "    print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "    print('Max sequence length for outputs:', max_decoder_seq_length)\n",
    "\n",
    "    input_token_index = dict(\n",
    "        [(char, i) for i, char in enumerate(input_characters)])\n",
    "    target_token_index = dict(\n",
    "        [(char, i) for i, char in enumerate(target_characters)])\n",
    "    encoder_input_data = np.zeros(\n",
    "        (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "        dtype='float32')\n",
    "    decoder_input_data = np.zeros(\n",
    "        (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "        dtype='float32')\n",
    "    decoder_target_data = np.zeros(\n",
    "        (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "        dtype='float32')\n",
    "\n",
    "    for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "        for t, char in enumerate(input_text):\n",
    "            encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "        for t, char in enumerate(target_text):\n",
    "            # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "            decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "            if t > 0:\n",
    "                # decoder_target_data will be ahead by one timestep\n",
    "                # and will not include the start character.\n",
    "                decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n",
    "    return encoder_input_data, decoder_input_data, decoder_target_data, \\\n",
    "            input_token_index, target_token_index, \\\n",
    "            num_encoder_tokens, num_decoder_tokens, \\\n",
    "            max_encoder_seq_length, max_decoder_seq_length, \\\n",
    "            input_texts, target_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traducción con secuencia mas larga (Notar el agregado de tab y enter):\n",
      "I got carded.\n",
      "\tOn m'a demandé ma carte d'identité pour vérifier mon âge.\n",
      "\n",
      "Number of samples: 10000\n",
      "Number of unique input tokens: 71\n",
      "Number of unique output tokens: 93\n",
      "Max sequence length for inputs: 16\n",
      "Max sequence length for outputs: 59\n"
     ]
    }
   ],
   "source": [
    "num_samples = 10000\n",
    "encoder_input_data, decoder_input_data, decoder_target_data, \\\n",
    "input_token_index, target_token_index, \\\n",
    "num_encoder_tokens, num_decoder_tokens,  \\\n",
    "max_encoder_seq_length, \\\n",
    "max_decoder_seq_length, \\\n",
    "input_texts, target_texts = get_data(num_samples = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idioma Ingles:\n",
      "Entrada encoder: (10000, 16, 71)\n",
      "Idioma frances:\n",
      "Entrada decoder: (10000, 59, 93)\n",
      "Salida decoder: (10000, 59, 93)\n"
     ]
    }
   ],
   "source": [
    "print('Idioma Ingles:')\n",
    "print('Entrada encoder:', encoder_input_data.shape)\n",
    "print('Idioma frances:')\n",
    "print('Entrada decoder:', decoder_input_data.shape)\n",
    "print('Salida decoder:', decoder_target_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cuál es la diferencia entre **decoder_input_data** y **decoder_target_data**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definicion del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estados:\n",
    "\n",
    "![lstm-states.png](lstm-states.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Encoder_Inputs (InputLayer)     (None, None, 71)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Dencoder_Inputs (InputLayer)    (None, None, 93)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Encoder_LSTM (LSTM)             [(None, 256), (None, 335872      Encoder_Inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Decoder_LSTM (LSTM)             [(None, None, 256),  358400      Dencoder_Inputs[0][0]            \n",
      "                                                                 Encoder_LSTM[0][1]               \n",
      "                                                                 Encoder_LSTM[0][2]               \n",
      "__________________________________________________________________________________________________\n",
      "Model_Output (Dense)            (None, None, 93)     23901       Decoder_LSTM[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 718,173\n",
      "Trainable params: 718,173\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, TimeDistributed\n",
    "# Estamos utilizando la Functional API\n",
    "\n",
    "# Esto es donde guardará el contexto\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens), name=\"Encoder_Inputs\") #num_encoder_tokens es la cantidad de features a la entrada\n",
    "encoder = LSTM(latent_dim, return_state=True, name=\"Encoder_LSTM\")\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens), name=\"Dencoder_Inputs\") #num_decoder_tokens es la cantidad de features a la entrada del decoder\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the \n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, name=\"Decoder_LSTM\")\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax', name='Model_Output')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![seq2seq-teacher-forcing.png](seq2seq-teacher-forcing.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamos el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      "8000/8000 [==============================] - 141s - loss: 0.9299 - val_loss: 0.9911\n",
      "Epoch 2/100\n",
      "8000/8000 [==============================] - 124s - loss: 0.7412 - val_loss: 0.7995\n",
      "Epoch 3/100\n",
      "8000/8000 [==============================] - 123s - loss: 0.6276 - val_loss: 0.7429\n",
      "Epoch 4/100\n",
      "8000/8000 [==============================] - 122s - loss: 0.5706 - val_loss: 0.6670\n",
      "Epoch 5/100\n",
      "8000/8000 [==============================] - 126s - loss: 0.5304 - val_loss: 0.6346\n",
      "Epoch 6/100\n",
      "8000/8000 [==============================] - 125s - loss: 0.4975 - val_loss: 0.6101\n",
      "Epoch 7/100\n",
      "8000/8000 [==============================] - 124s - loss: 0.4700 - val_loss: 0.5844\n",
      "Epoch 8/100\n",
      "8000/8000 [==============================] - 122s - loss: 0.4462 - val_loss: 0.5722\n",
      "Epoch 9/100\n",
      "8000/8000 [==============================] - 122s - loss: 0.4258 - val_loss: 0.5538\n",
      "Epoch 10/100\n",
      "8000/8000 [==============================] - 122s - loss: 0.4079 - val_loss: 0.5344\n",
      "Epoch 11/100\n",
      "8000/8000 [==============================] - 123s - loss: 0.3914 - val_loss: 0.5283\n",
      "Epoch 12/100\n",
      "8000/8000 [==============================] - 143s - loss: 0.3759 - val_loss: 0.5175\n",
      "Epoch 13/100\n",
      "8000/8000 [==============================] - 139s - loss: 0.3622 - val_loss: 0.5099\n",
      "Epoch 14/100\n",
      "8000/8000 [==============================] - 131s - loss: 0.3496 - val_loss: 0.5071\n",
      "Epoch 15/100\n",
      "8000/8000 [==============================] - 131s - loss: 0.3370 - val_loss: 0.4992\n",
      "Epoch 16/100\n",
      "8000/8000 [==============================] - 143s - loss: 0.3257 - val_loss: 0.4974\n",
      "Epoch 17/100\n",
      "8000/8000 [==============================] - 155s - loss: 0.3145 - val_loss: 0.4918\n",
      "Epoch 18/100\n",
      "8000/8000 [==============================] - 136s - loss: 0.3042 - val_loss: 0.4896\n",
      "Epoch 19/100\n",
      "8000/8000 [==============================] - 120s - loss: 0.2946 - val_loss: 0.4853\n",
      "Epoch 20/100\n",
      "8000/8000 [==============================] - 120s - loss: 0.2850 - val_loss: 0.4882\n",
      "Epoch 21/100\n",
      "8000/8000 [==============================] - 120s - loss: 0.2758 - val_loss: 0.4884\n",
      "Epoch 22/100\n",
      "8000/8000 [==============================] - 121s - loss: 0.2671 - val_loss: 0.4901\n",
      "Epoch 23/100\n",
      "8000/8000 [==============================] - 120s - loss: 0.2590 - val_loss: 0.4858\n",
      "Epoch 24/100\n",
      "8000/8000 [==============================] - 119s - loss: 0.2509 - val_loss: 0.4910\n",
      "Epoch 25/100\n",
      "8000/8000 [==============================] - 119s - loss: 0.2433 - val_loss: 0.4897\n",
      "Epoch 26/100\n",
      "8000/8000 [==============================] - 119s - loss: 0.2362 - val_loss: 0.4947\n",
      "Epoch 27/100\n",
      "8000/8000 [==============================] - 119s - loss: 0.2292 - val_loss: 0.4931\n",
      "Epoch 28/100\n",
      "8000/8000 [==============================] - 118s - loss: 0.2224 - val_loss: 0.5013\n",
      "Epoch 29/100\n",
      "8000/8000 [==============================] - 119s - loss: 0.2157 - val_loss: 0.4978\n",
      "Epoch 30/100\n",
      "8000/8000 [==============================] - 118s - loss: 0.2100 - val_loss: 0.5001\n",
      "Epoch 31/100\n",
      "8000/8000 [==============================] - 119s - loss: 0.2042 - val_loss: 0.5036\n",
      "Epoch 32/100\n",
      "8000/8000 [==============================] - 119s - loss: 0.1984 - val_loss: 0.5086\n",
      "Epoch 33/100\n",
      "8000/8000 [==============================] - 119s - loss: 0.1929 - val_loss: 0.5118\n",
      "Epoch 34/100\n",
      "8000/8000 [==============================] - 119s - loss: 0.1875 - val_loss: 0.5194\n",
      "Epoch 35/100\n",
      "8000/8000 [==============================] - 120s - loss: 0.1829 - val_loss: 0.5194\n",
      "Epoch 36/100\n",
      "8000/8000 [==============================] - 119s - loss: 0.1779 - val_loss: 0.5247\n",
      "Epoch 37/100\n",
      "8000/8000 [==============================] - 119s - loss: 0.1731 - val_loss: 0.5255\n",
      "Epoch 38/100\n",
      "8000/8000 [==============================] - 119s - loss: 0.1689 - val_loss: 0.5360\n",
      "Epoch 39/100\n",
      "8000/8000 [==============================] - 119s - loss: 0.1645 - val_loss: 0.5457\n",
      "Epoch 40/100\n",
      "8000/8000 [==============================] - 119s - loss: 0.1603 - val_loss: 0.5447\n",
      "Epoch 41/100\n",
      "8000/8000 [==============================] - 119s - loss: 0.1566 - val_loss: 0.5460\n",
      "Epoch 42/100\n",
      "8000/8000 [==============================] - 120s - loss: 0.1528 - val_loss: 0.5554\n",
      "Epoch 43/100\n",
      "8000/8000 [==============================] - 119s - loss: 0.1490 - val_loss: 0.5557\n",
      "Epoch 44/100\n",
      "8000/8000 [==============================] - 119s - loss: 0.1456 - val_loss: 0.5637\n",
      "Epoch 45/100\n",
      "8000/8000 [==============================] - 119s - loss: 0.1422 - val_loss: 0.5655\n",
      "Epoch 46/100\n",
      "8000/8000 [==============================] - 119s - loss: 0.1389 - val_loss: 0.5704\n",
      "Epoch 47/100\n",
      "8000/8000 [==============================] - 119s - loss: 0.1362 - val_loss: 0.5813\n",
      "Epoch 48/100\n",
      "8000/8000 [==============================] - 119s - loss: 0.1331 - val_loss: 0.5801\n",
      "Epoch 49/100\n",
      "8000/8000 [==============================] - 119s - loss: 0.1303 - val_loss: 0.5841\n",
      "Epoch 50/100\n",
      "8000/8000 [==============================] - 119s - loss: 0.1273 - val_loss: 0.5897\n",
      "Epoch 51/100\n",
      "8000/8000 [==============================] - 119s - loss: 0.1245 - val_loss: 0.5960\n",
      "Epoch 52/100\n",
      "8000/8000 [==============================] - 119s - loss: 0.1224 - val_loss: 0.6017\n",
      "Epoch 53/100\n",
      "8000/8000 [==============================] - 118s - loss: 0.1199 - val_loss: 0.6050\n",
      "Epoch 54/100\n",
      "8000/8000 [==============================] - 119s - loss: 0.1174 - val_loss: 0.6049\n",
      "Epoch 55/100\n",
      "8000/8000 [==============================] - 118s - loss: 0.1155 - val_loss: 0.6108\n",
      "Epoch 56/100\n",
      "8000/8000 [==============================] - 118s - loss: 0.1127 - val_loss: 0.6096\n",
      "Epoch 57/100\n",
      "8000/8000 [==============================] - 119s - loss: 0.1109 - val_loss: 0.6179\n",
      "Epoch 58/100\n",
      "8000/8000 [==============================] - 119s - loss: 0.1088 - val_loss: 0.6201\n",
      "Epoch 59/100\n",
      "8000/8000 [==============================] - 119s - loss: 0.1070 - val_loss: 0.6327\n",
      "Epoch 60/100\n",
      "8000/8000 [==============================] - 119s - loss: 0.1048 - val_loss: 0.6346\n",
      "Epoch 61/100\n",
      "8000/8000 [==============================] - 118s - loss: 0.1030 - val_loss: 0.6329\n",
      "Epoch 62/100\n",
      "8000/8000 [==============================] - 119s - loss: 0.1013 - val_loss: 0.6408\n",
      "Epoch 63/100\n",
      "8000/8000 [==============================] - 119s - loss: 0.0995 - val_loss: 0.6531\n",
      "Epoch 64/100\n",
      "8000/8000 [==============================] - 118s - loss: 0.0980 - val_loss: 0.6477\n",
      "Epoch 65/100\n",
      "8000/8000 [==============================] - 119s - loss: 0.0961 - val_loss: 0.6520\n",
      "Epoch 66/100\n",
      "8000/8000 [==============================] - 119s - loss: 0.0943 - val_loss: 0.6604\n",
      "Epoch 67/100\n",
      "8000/8000 [==============================] - 119s - loss: 0.0931 - val_loss: 0.6697\n",
      "Epoch 68/100\n",
      "8000/8000 [==============================] - 119s - loss: 0.0914 - val_loss: 0.6630\n",
      "Epoch 69/100\n",
      "8000/8000 [==============================] - 119s - loss: 0.0901 - val_loss: 0.6702\n",
      "Epoch 70/100\n",
      "8000/8000 [==============================] - 119s - loss: 0.0887 - val_loss: 0.6801\n",
      "Epoch 71/100\n",
      "8000/8000 [==============================] - 119s - loss: 0.0872 - val_loss: 0.6752\n",
      "Epoch 72/100\n",
      "8000/8000 [==============================] - 119s - loss: 0.0857 - val_loss: 0.6839\n",
      "Epoch 73/100\n",
      "8000/8000 [==============================] - 118s - loss: 0.0847 - val_loss: 0.6878\n",
      "Epoch 74/100\n",
      "8000/8000 [==============================] - 119s - loss: 0.0833 - val_loss: 0.6936\n",
      "Epoch 75/100\n",
      "8000/8000 [==============================] - 119s - loss: 0.0819 - val_loss: 0.6937\n",
      "Epoch 76/100\n",
      "8000/8000 [==============================] - 118s - loss: 0.0805 - val_loss: 0.6956\n",
      "Epoch 77/100\n",
      "8000/8000 [==============================] - 119s - loss: 0.0797 - val_loss: 0.6979\n",
      "Epoch 78/100\n",
      "8000/8000 [==============================] - 119s - loss: 0.0781 - val_loss: 0.7056\n",
      "Epoch 79/100\n",
      "8000/8000 [==============================] - 118s - loss: 0.0769 - val_loss: 0.7117\n",
      "Epoch 80/100\n",
      "8000/8000 [==============================] - 119s - loss: 0.0759 - val_loss: 0.7173\n",
      "Epoch 81/100\n",
      "8000/8000 [==============================] - 119s - loss: 0.0745 - val_loss: 0.7128\n",
      "Epoch 82/100\n",
      "8000/8000 [==============================] - 119s - loss: 0.0739 - val_loss: 0.7191\n",
      "Epoch 83/100\n",
      "8000/8000 [==============================] - 119s - loss: 0.0726 - val_loss: 0.7243\n",
      "Epoch 84/100\n",
      "8000/8000 [==============================] - 118s - loss: 0.0715 - val_loss: 0.7271\n",
      "Epoch 85/100\n",
      "8000/8000 [==============================] - 119s - loss: 0.0704 - val_loss: 0.7300\n",
      "Epoch 86/100\n",
      "8000/8000 [==============================] - 119s - loss: 0.0695 - val_loss: 0.7350\n",
      "Epoch 87/100\n",
      "8000/8000 [==============================] - 119s - loss: 0.0685 - val_loss: 0.7358\n",
      "Epoch 88/100\n",
      "8000/8000 [==============================] - 119s - loss: 0.0675 - val_loss: 0.7351\n",
      "Epoch 89/100\n",
      "8000/8000 [==============================] - 118s - loss: 0.0664 - val_loss: 0.7462\n",
      "Epoch 90/100\n",
      "8000/8000 [==============================] - 119s - loss: 0.0659 - val_loss: 0.7471\n",
      "Epoch 91/100\n",
      "8000/8000 [==============================] - 119s - loss: 0.0646 - val_loss: 0.7530\n",
      "Epoch 92/100\n",
      "8000/8000 [==============================] - 119s - loss: 0.0639 - val_loss: 0.7585\n",
      "Epoch 93/100\n",
      "8000/8000 [==============================] - 119s - loss: 0.0630 - val_loss: 0.7622\n",
      "Epoch 94/100\n",
      "8000/8000 [==============================] - 119s - loss: 0.0620 - val_loss: 0.7669\n",
      "Epoch 95/100\n",
      "8000/8000 [==============================] - 119s - loss: 0.0615 - val_loss: 0.7662\n",
      "Epoch 96/100\n",
      "8000/8000 [==============================] - 119s - loss: 0.0604 - val_loss: 0.7682\n",
      "Epoch 97/100\n",
      "8000/8000 [==============================] - 119s - loss: 0.0595 - val_loss: 0.7727\n",
      "Epoch 98/100\n",
      "8000/8000 [==============================] - 119s - loss: 0.0588 - val_loss: 0.7746\n",
      "Epoch 99/100\n",
      "8000/8000 [==============================] - 119s - loss: 0.0581 - val_loss: 0.7707\n",
      "Epoch 100/100\n",
      "8000/8000 [==============================] - 119s - loss: 0.0573 - val_loss: 0.7756\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11a5a8b70>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run training\n",
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 100  # Number of epochs to train for.\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights('seq2seq_enc_dec_model.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'Dencoder_Inputs_10:0' shape=(?, ?, 93) dtype=float32>, <tf.Tensor 'State_input_h:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'State_input_c:0' shape=(?, 256) dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "print([decoder_inputs] + decoder_states_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'Model_Output_4/truediv:0' shape=(?, ?, 93) dtype=float32>, <tf.Tensor 'Decoder_LSTM_11/while/Exit_2:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'Decoder_LSTM_11/while/Exit_3:0' shape=(?, 256) dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "print([decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,), name=\"State_input_h\")\n",
    "decoder_state_input_c = Input(shape=(latent_dim,), name=\"State_input_c\")\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Encoder_Inputs (InputLayer)  (None, None, 71)          0         \n",
      "_________________________________________________________________\n",
      "Encoder_LSTM (LSTM)          [(None, 256), (None, 256) 335872    \n",
      "=================================================================\n",
      "Total params: 335,872\n",
      "Trainable params: 335,872\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "Dencoder_Inputs (InputLayer)     (None, None, 93)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "State_input_h (InputLayer)       (None, 256)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "State_input_c (InputLayer)       (None, 256)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "Decoder_LSTM (LSTM)              [(None, None, 256), ( 358400      Dencoder_Inputs[0][0]            \n",
      "                                                                   State_input_h[0][0]              \n",
      "                                                                   State_input_c[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "Model_Output (Dense)             (None, None, 93)      23901       Decoder_LSTM[3][0]               \n",
      "====================================================================================================\n",
      "Total params: 382,301\n",
      "Trainable params: 382,301\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: Go.\n",
      "Decoded sentence: Va !\n",
      "\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: Courez !\n",
      "\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: Courez !\n",
      "\n",
      "-\n",
      "Input sentence: Wow!\n",
      "Decoded sentence: Ça alors !\n",
      "\n",
      "-\n",
      "Input sentence: Fire!\n",
      "Decoded sentence: Au feu !\n",
      "\n",
      "-\n",
      "Input sentence: Help!\n",
      "Decoded sentence: À l'aide !\n",
      "\n",
      "-\n",
      "Input sentence: Jump.\n",
      "Decoded sentence: Saute.\n",
      "\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Arrête-toi !\n",
      "\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Arrête-toi !\n",
      "\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Arrête-toi !\n",
      "\n",
      "-\n",
      "Input sentence: Wait!\n",
      "Decoded sentence: Attendez !\n",
      "\n",
      "-\n",
      "Input sentence: Wait!\n",
      "Decoded sentence: Attendez !\n",
      "\n",
      "-\n",
      "Input sentence: I see.\n",
      "Decoded sentence: Je te les aimé.\n",
      "\n",
      "-\n",
      "Input sentence: I try.\n",
      "Decoded sentence: J'essaye.\n",
      "\n",
      "-\n",
      "Input sentence: I won!\n",
      "Decoded sentence: J'y suis allée deux fois.\n",
      "\n",
      "-\n",
      "Input sentence: I won!\n",
      "Decoded sentence: J'y suis allée deux fois.\n",
      "\n",
      "-\n",
      "Input sentence: Oh no!\n",
      "Decoded sentence: Oh non !\n",
      "\n",
      "-\n",
      "Input sentence: Attack!\n",
      "Decoded sentence: Attaque !\n",
      "\n",
      "-\n",
      "Input sentence: Attack!\n",
      "Decoded sentence: Attaque !\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: Merci !\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: Merci !\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: Merci !\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: Merci !\n",
      "\n",
      "-\n",
      "Input sentence: Get up.\n",
      "Decoded sentence: Lève-toi.\n",
      "\n",
      "-\n",
      "Input sentence: Got it!\n",
      "Decoded sentence: Compris !\n",
      "\n",
      "-\n",
      "Input sentence: Got it!\n",
      "Decoded sentence: Compris !\n",
      "\n",
      "-\n",
      "Input sentence: Got it?\n",
      "Decoded sentence: Compris ?\n",
      "\n",
      "-\n",
      "Input sentence: Got it?\n",
      "Decoded sentence: Compris ?\n",
      "\n",
      "-\n",
      "Input sentence: Got it?\n",
      "Decoded sentence: Compris ?\n",
      "\n",
      "-\n",
      "Input sentence: Hop in.\n",
      "Decoded sentence: Montez.\n",
      "\n",
      "-\n",
      "Input sentence: Hop in.\n",
      "Decoded sentence: Montez.\n",
      "\n",
      "-\n",
      "Input sentence: Hug me.\n",
      "Decoded sentence: Serre-moi dans tes bras !\n",
      "\n",
      "-\n",
      "Input sentence: Hug me.\n",
      "Decoded sentence: Serre-moi dans tes bras !\n",
      "\n",
      "-\n",
      "Input sentence: I fell.\n",
      "Decoded sentence: Je suis tombée.\n",
      "\n",
      "-\n",
      "Input sentence: I fell.\n",
      "Decoded sentence: Je suis tombée.\n",
      "\n",
      "-\n",
      "Input sentence: I know.\n",
      "Decoded sentence: Je sais.\n",
      "\n",
      "-\n",
      "Input sentence: I left.\n",
      "Decoded sentence: Je suis partie.\n",
      "\n",
      "-\n",
      "Input sentence: I left.\n",
      "Decoded sentence: Je suis partie.\n",
      "\n",
      "-\n",
      "Input sentence: I lost.\n",
      "Decoded sentence: J'ai perdu.\n",
      "\n",
      "-\n",
      "Input sentence: I'm 19.\n",
      "Decoded sentence: Je suis gavée.\n",
      "\n",
      "-\n",
      "Input sentence: I'm OK.\n",
      "Decoded sentence: Je suis alitée.\n",
      "\n",
      "-\n",
      "Input sentence: I'm OK.\n",
      "Decoded sentence: Je suis alitée.\n",
      "\n",
      "-\n",
      "Input sentence: Listen.\n",
      "Decoded sentence: Écoutez !\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: C'est exclu !\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: C'est exclu !\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: C'est exclu !\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: C'est exclu !\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: C'est exclu !\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: C'est exclu !\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: C'est exclu !\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: C'est exclu !\n",
      "\n",
      "-\n",
      "Input sentence: Really?\n",
      "Decoded sentence: Vraiment ?\n",
      "\n",
      "-\n",
      "Input sentence: Really?\n",
      "Decoded sentence: Vraiment ?\n",
      "\n",
      "-\n",
      "Input sentence: Really?\n",
      "Decoded sentence: Vraiment ?\n",
      "\n",
      "-\n",
      "Input sentence: Thanks.\n",
      "Decoded sentence: Merci !\n",
      "\n",
      "-\n",
      "Input sentence: We try.\n",
      "Decoded sentence: On essaye.\n",
      "\n",
      "-\n",
      "Input sentence: We won.\n",
      "Decoded sentence: Nous l'avons emporté.\n",
      "\n",
      "-\n",
      "Input sentence: We won.\n",
      "Decoded sentence: Nous l'avons emporté.\n",
      "\n",
      "-\n",
      "Input sentence: We won.\n",
      "Decoded sentence: Nous l'avons emporté.\n",
      "\n",
      "-\n",
      "Input sentence: We won.\n",
      "Decoded sentence: Nous l'avons emporté.\n",
      "\n",
      "-\n",
      "Input sentence: Ask Tom.\n",
      "Decoded sentence: Demande à Tom.\n",
      "\n",
      "-\n",
      "Input sentence: Awesome!\n",
      "Decoded sentence: Fantastique !\n",
      "\n",
      "-\n",
      "Input sentence: Be calm.\n",
      "Decoded sentence: Soyez calme !\n",
      "\n",
      "-\n",
      "Input sentence: Be calm.\n",
      "Decoded sentence: Soyez calme !\n",
      "\n",
      "-\n",
      "Input sentence: Be calm.\n",
      "Decoded sentence: Soyez calme !\n",
      "\n",
      "-\n",
      "Input sentence: Be cool.\n",
      "Decoded sentence: Sois détendu !\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: Soyez équitables !\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: Soyez équitables !\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: Soyez équitables !\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: Soyez équitables !\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: Soyez équitables !\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: Soyez équitables !\n",
      "\n",
      "-\n",
      "Input sentence: Be kind.\n",
      "Decoded sentence: Sois gentil.\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: Soyez gentil !\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: Soyez gentil !\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: Soyez gentil !\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: Soyez gentil !\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: Soyez gentil !\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: Soyez gentil !\n",
      "\n",
      "-\n",
      "Input sentence: Beat it.\n",
      "Decoded sentence: Dégage !\n",
      "\n",
      "-\n",
      "Input sentence: Call me.\n",
      "Decoded sentence: Appelle-moi !\n",
      "\n",
      "-\n",
      "Input sentence: Call me.\n",
      "Decoded sentence: Appelle-moi !\n",
      "\n",
      "-\n",
      "Input sentence: Call us.\n",
      "Decoded sentence: Appelez-nous !\n",
      "\n",
      "-\n",
      "Input sentence: Call us.\n",
      "Decoded sentence: Appelez-nous !\n",
      "\n",
      "-\n",
      "Input sentence: Come in.\n",
      "Decoded sentence: Entrez !\n",
      "\n",
      "-\n",
      "Input sentence: Come in.\n",
      "Decoded sentence: Entrez !\n",
      "\n",
      "-\n",
      "Input sentence: Come in.\n",
      "Decoded sentence: Entrez !\n",
      "\n",
      "-\n",
      "Input sentence: Come in.\n",
      "Decoded sentence: Entrez !\n",
      "\n",
      "-\n",
      "Input sentence: Come on!\n",
      "Decoded sentence: Allez !\n",
      "\n",
      "-\n",
      "Input sentence: Come on.\n",
      "Decoded sentence: Venez !\n",
      "\n",
      "-\n",
      "Input sentence: Come on.\n",
      "Decoded sentence: Venez !\n",
      "\n",
      "-\n",
      "Input sentence: Come on.\n",
      "Decoded sentence: Venez !\n",
      "\n",
      "-\n",
      "Input sentence: Drop it!\n",
      "Decoded sentence: Laissez tomber !\n",
      "\n",
      "-\n",
      "Input sentence: Drop it!\n",
      "Decoded sentence: Laissez tomber !\n",
      "\n",
      "-\n",
      "Input sentence: Drop it!\n",
      "Decoded sentence: Laissez tomber !\n",
      "\n",
      "-\n",
      "Input sentence: Drop it!\n",
      "Decoded sentence: Laissez tomber !\n",
      "\n",
      "-\n",
      "Input sentence: Get out!\n",
      "Decoded sentence: Sortez !\n",
      "\n",
      "-\n",
      "Input sentence: Get out!\n",
      "Decoded sentence: Sortez !\n",
      "\n",
      "-\n",
      "Input sentence: Get out!\n",
      "Decoded sentence: Sortez !\n",
      "\n",
      "-\n",
      "Input sentence: Get out.\n",
      "Decoded sentence: Sors.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(100):\n",
    "    # Take one sequence (part of the training test)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Se esta usando argmax. Se podría mejorar? Como?\n",
    "- Viterbi?\n",
    "- Stocastich Beam Search\n",
    "- Simulated Annealing\n",
    "- Greedy Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comentar opciones con words embeddings, GRU y sin Teach forcing\n",
    "https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interesante:\n",
    "\n",
    "Facebook saco un modelo de traducción con CNNs:\n",
    "\n",
    "https://s3.amazonaws.com/fairseq/papers/convolutional-sequence-to-sequence-learning.pdf\n",
    "\n",
    "https://datawarrior.wordpress.com/2017/05/11/convnet-seq2seq-for-machine-translation/\n",
    "\n",
    "Paper de Google de 2016 sobre traducción:\n",
    "\n",
    "https://arxiv.org/abs/1609.08144"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
